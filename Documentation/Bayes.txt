Bayes for robotics?

Deals with probabilities involving evidence.

Humans are bad with probabilities, and especially bad at probabilities involving prior likelihoods.  We're also bad with logic.

Imagine a statement like "Most robotics engineers speak English".  This means that if you choose a robotics engineer at random, the probability that they speak English is greater than 0.5.

However, this statement certainly does not mean that most English speakers are robotics engineers!  If you chose a random person who speaks English, they almost certainly won't be!  This is because robotics engineers make up only a small proportion of any population.

This is analogous to the inverse proposition in logic:
A -> B does not mean that B -> A

The problem is that many people will hear or read a statistic such as the one above and wrongly conclude that the converse (B->A) is true.

Bayesian statistics and probability allows you to deal with these "conditional probabilities".  Bayesian probability is a nice combination of logic and statistics.

We can write the conditional probability of something as:

p(A|B)

This means "the probability of A, given B".

The Bayes theorem states that:

p(A|B) = p(B|A) x p(A) / p(B)

also written in terms of H (hypothesis) and E (evidence),  like so:

p(H|E) = p(E|H) x p(H) / p(E)

As you can see, the Bayes theorem lets you "flip around" the conditional probability (like finding B -> A given A -> B), provided you know the independent probabilities of A and B.

Let's work through a example:


We'll make it quantitative by saying that 70% of robotics engineers speak English.  We'll use R and E instead of A and B to make it clearer.

R is the condition of being a robotics engineer.
E is the condition of being an English speaker.
E|R refers to someone being an English speaker given that they are a robotics engineer.
R|E means the condition of someone being a robotics engineer given that they speak English (not the same thing at all as E|R!).

"p(E|R) = 0.7" equates to the statement "70% of robotics engineers speak English"

p(R|E) (the probability of any English speaker being a robotics engineer) is what we are trying to determine.

To work out the probability of a random English speaker being a robotics engineer, we need to know the independent probabilities of someone being (a) a robotics engineer, and (b) an English speaker.

Around a billion people can speak English globally (though not necessarily as their first language), out of a total world population of approaching 8 billion, so the probability p(E) is:

p(E) = 1 billion / 8 billion
     = 0.125

Let's (arbitrarily) suppose that 1% of people are robotics engineers, so:

p(R) = 0.01

We know already that p(E|R) = 0.7

So, p(R|E) = p(E|R) x p(R) / p(E)
           = 0.7 x 0.01 / 0.13
           = 0.05

In other words, the probability of a random English speaker being a robotics engineer is 5% - not much greater than the probability of any random person being a robotics engineer, and certainly far from a majority. It makes sense that this probability is so low, because robotics engineers are generally infrequent/a rare breed.

TODO: other examples, such as forensic evidence...

--

Combining Probabilities (Naive Bayes technique)

Often, you want to combine the probabilities of several independent pieces of evdence to derive an overall probability ("certainty").  This could be useful in robotics for example in determining where on the soccer field a robot is, based on compass reading, downward-pointing colour sensors, and input from the computer vision system.  This approach lets you reason numerically about discrete categories, e.g. ""

If a and b are independent probabilites, they combine like so:

ab / (ab + (1-a)(1-b))

For three terms, the combined probability is:

abc / (abc + (1-a)(1-b)(1-c))


In Tcl, this can be generalised like so, using some string manipulation to derive the mathematical expression:

proc combined_probability p_list {
	set abc... "[join $p_list *]"
	set formula "${abc...} / (${abc...} + (1-[join $p_list )*(1-]))"
	puts $formula
	expr $formula
}

Behaviours to note:

With probabilities all less than 0.5, the result will approach 0 as you add more probabilities.

With probabilities all greater than 0.5, the result approaches 1 as you add more.

With all probabilities equal to 0.5, the combined probability will also be 0.5.  In fact, any 0.5 probabilities don't contribute to the combined probability.

This allows the system to become very confident that its conclusion is correct, and the more evidence the better. It also helps when classifying, as the likely hypotheses tend to be very strongly supported and the unlikely ones become vanishingly unlikely.  That is, there is usually a clear winner.

The formula can be written like so, for easier implementation:

1/p-1 = (1-p1)(1-p2)...(1-pN) / p1 x p2 x ... x pN

Floating-point underflow can be a problem with these calculations, so sometimes the log of the terms is used instead:

p = 1 / (1 + e^h)

where:

h = sum(ln(1-pN) - ln(pN))
